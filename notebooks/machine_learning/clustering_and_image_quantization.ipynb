{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Color Quantization Using K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we'll look at color quantization. This is a way to reduce the storage space of an image by cutting down on how many colors are used. Instead of storing a color for each pixel, you can store a reference to a dictionary containing the colors. The following code sets up the example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Color Quantization Example\n",
    "#\n",
    "# Authors: Robert Layton <robertlayton@gmail.com>\n",
    "#          Olivier Grisel <olivier.grisel@ensta.org>\n",
    "#          Mathieu Blondel <mathieu@mblondel.org>\n",
    "#\n",
    "# License: BSD 3 clause\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances_argmin\n",
    "from sklearn.datasets import load_sample_image\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "class ColorQuant():\n",
    "    \n",
    "    def _generate_random_codebook(self, n_colors):\n",
    "        \"\"\" Generate a codebook and labels for the image using nearest neighbors on n_colors pixels\n",
    "        chosen at random from the image.\"\"\"\n",
    "        codebook_random = shuffle(self.image_array, random_state=self.random_state)[:n_colors + 1]\n",
    "        labels_random = pairwise_distances_argmin(codebook_random,\n",
    "                                          self.image_array,\n",
    "                                          axis=0)\n",
    "        return [codebook_random, labels_random]\n",
    "    \n",
    "    def _generate_kmeans_codebook(self, n_colors):\n",
    "        \"\"\" Generate a codebook and labels for the image using nearest kmeans with n_colors\n",
    "        clusters.\"\"\"\n",
    "        kmeans = KMeans(n_clusters=n_colors, random_state=self.random_state).fit(self.image_array_sample)\n",
    "\n",
    "        labels_kmeans = kmeans.predict(self.image_array)\n",
    "        \n",
    "        codebook_kmeans = kmeans.cluster_centers_\n",
    "        return [codebook_kmeans, labels_kmeans]\n",
    "    \n",
    "    def _recreate_image(self, codebook, labels):\n",
    "        \"\"\"Recreate the (compressed) image from the code book & labels\"\"\"\n",
    "        d = codebook.shape[1]\n",
    "        image = np.zeros((self.w, self.h, d))\n",
    "        label_idx = 0\n",
    "        for i in range(self.w):\n",
    "            for j in range(self.h):\n",
    "                image[i][j] = codebook[labels[label_idx]]\n",
    "                label_idx += 1\n",
    "        return image\n",
    "    \n",
    "    def _compress_randomly(self, n_colors):\n",
    "        \"\"\" Compress the image using randomly selected colors from the image pixels. \"\"\"\n",
    "        codebook_random, labels_random = self._generate_random_codebook(n_colors)\n",
    "        image_random = self._recreate_image(codebook_random, labels_random)\n",
    "        return image_random\n",
    "    \n",
    "    def _compress_using_kmeans(self, n_colors):\n",
    "        \"\"\" Compress the image using kmeans-selected colors. \"\"\"\n",
    "        codebook_kmeans, labels_kmeans = self._generate_kmeans_codebook(n_colors)\n",
    "        image_kmeans = self._recreate_image(codebook_kmeans, labels_kmeans)\n",
    "        return image_kmeans\n",
    "    \n",
    "    def compress(self, n_colors=64):\n",
    "        \"\"\" Compress the image into n_colors using both random and kmeans color selection. Plot both\n",
    "        compressed images and the original. \"\"\"\n",
    "        \n",
    "        image_kmeans = self._compress_using_kmeans(n_colors)\n",
    "        image_random = self._compress_randomly(n_colors)\n",
    "        \n",
    "        plt.figure(1)\n",
    "        plt.clf()\n",
    "        ax = plt.axes([0, 0, 1, 1])\n",
    "        plt.axis('off')\n",
    "        plt.title('Original image (96,615 colors)')\n",
    "        plt.imshow(self.image_original)\n",
    "\n",
    "        plt.figure(2)\n",
    "        plt.clf()\n",
    "        ax = plt.axes([0, 0, 1, 1])\n",
    "        plt.axis('off')\n",
    "        plt.title('Quantized image ({} colors, K-Means)'.format(n_colors))\n",
    "        plt.imshow(image_kmeans)\n",
    "\n",
    "        plt.figure(3)\n",
    "        plt.clf()\n",
    "        ax = plt.axes([0, 0, 1, 1])\n",
    "        plt.axis('off')\n",
    "        plt.title('Quantized image ({} colors, Random)'.format(n_colors))\n",
    "        plt.imshow(image_random)\n",
    "        plt.show()\n",
    "        \n",
    "    def show_image(self, image):\n",
    "        plt.figure(1)\n",
    "        plt.clf()\n",
    "        ax = plt.axes([0, 0, 1, 1])\n",
    "        plt.axis('off')\n",
    "        plt.imshow(image)\n",
    "    \n",
    "    def __init__(self, random_state = 0):\n",
    "        \n",
    "        self.random_state = random_state\n",
    "        \n",
    "        # Load the Summer Palace photo\n",
    "        image_original = load_sample_image(\"china.jpg\")\n",
    "\n",
    "        # Convert to floats instead of the default 8 bits integer coding\n",
    "        image_original = np.array(image_original, dtype=np.float64) / 255\n",
    "\n",
    "        # Transform to a 2D numpy array and sample at random for KMeans\n",
    "        w, h, d = original_shape = tuple(image_original.shape)\n",
    "        image_array = np.reshape(image_original, (w * h, d))\n",
    "        image_array_sample = shuffle(image_array, random_state=0)[:1000]\n",
    "        \n",
    "        # Assign attributes\n",
    "        self.image_array_sample = image_array_sample\n",
    "        self.image_original = image_original\n",
    "        self.image_array = image_array\n",
    "        self.w = w\n",
    "        self.h = h\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at how it works. First let's see the original photo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_CQ = ColorQuant()\n",
    "example_CQ.show_image(example_CQ.image_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try compressing it. There are a lot of unique colors in the original image (almost 10,000). Suppose we wanted to use only 64? How should we choose them? Probably the simplest thing to do would be to pick 64 colors from the picture at random, and then assign to each pixel whichever of these 64 is the closest to the original color. We can do better than that, though, by using some of the clustering techniques we looked at earlier.\n",
    "\n",
    "Look at the sky in this photo - I see a gradient of pretty similar colors. It would be good if we could make sure we got at least one representative blue hue in our set of 64, maybe even 2 or 3. If we could find the mean of the blue, that might make a good choice. We can use KMeans to try to find this. The following line picks 64 colors at random and compares the result to doing KMeans on 64 clusters, using each pixel as a data point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_CQ.compress(64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we expected, the KMeans version looks better. Try running it again with a different number of colors. What's the minimum you need before it starts to look really bad? What about the minimum so that you can't tell the difference between the compressed and original image?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import cluster, datasets, mixture\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from itertools import cycle, islice\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# ============\n",
    "# Generate datasets. We choose the size big enough to see the scalability\n",
    "# of the algorithms, but not too big to avoid too long running times\n",
    "# ============\n",
    "n_samples = 1500\n",
    "noisy_circles = datasets.make_circles(n_samples=n_samples, factor=.5,\n",
    "                                      noise=.05)\n",
    "noisy_moons = datasets.make_moons(n_samples=n_samples, noise=.05)\n",
    "blobs = datasets.make_blobs(n_samples=n_samples, random_state=8)\n",
    "no_structure = np.random.rand(n_samples, 2), None\n",
    "\n",
    "# Anisotropicly distributed data\n",
    "random_state = 170\n",
    "X, y = datasets.make_blobs(n_samples=n_samples, random_state=random_state)\n",
    "transformation = [[0.6, -0.6], [-0.4, 0.8]]\n",
    "X_aniso = np.dot(X, transformation)\n",
    "aniso = (X_aniso, y)\n",
    "\n",
    "# blobs with varied variances\n",
    "varied = datasets.make_blobs(n_samples=n_samples,\n",
    "                             cluster_std=[1.0, 2.5, 0.5],\n",
    "                             random_state=random_state)\n",
    "\n",
    "# ============\n",
    "# Set up cluster parameters\n",
    "# ============\n",
    "plt.figure(figsize=(9 * 2 + 3, 12.5))\n",
    "plt.subplots_adjust(left=.02, right=.98, bottom=.001, top=.96, wspace=.05,\n",
    "                    hspace=.01)\n",
    "\n",
    "plot_num = 1\n",
    "\n",
    "default_base = {'quantile': .3,\n",
    "                'eps': .3,\n",
    "                'damping': .9,\n",
    "                'preference': -200,\n",
    "                'n_neighbors': 10,\n",
    "                'n_clusters': 3}\n",
    "\n",
    "datasets = [\n",
    "    (noisy_circles, {'damping': .77, 'preference': -240,\n",
    "                     'quantile': .2, 'n_clusters': 2}),\n",
    "    (noisy_moons, {'damping': .75, 'preference': -220, 'n_clusters': 2}),\n",
    "    (varied, {'eps': .18, 'n_neighbors': 2}),\n",
    "    (aniso, {'eps': .15, 'n_neighbors': 2}),\n",
    "    (blobs, {}),\n",
    "    (no_structure, {})]\n",
    "\n",
    "for i_dataset, (dataset, algo_params) in enumerate(datasets):\n",
    "    # update parameters with dataset-specific values\n",
    "    params = default_base.copy()\n",
    "    params.update(algo_params)\n",
    "\n",
    "    X, y = dataset\n",
    "\n",
    "    # normalize dataset for easier parameter selection\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "\n",
    "    # estimate bandwidth for mean shift\n",
    "    bandwidth = cluster.estimate_bandwidth(X, quantile=params['quantile'])\n",
    "\n",
    "    # connectivity matrix for structured Ward\n",
    "    connectivity = kneighbors_graph(\n",
    "        X, n_neighbors=params['n_neighbors'], include_self=False)\n",
    "    # make connectivity symmetric\n",
    "    connectivity = 0.5 * (connectivity + connectivity.T)\n",
    "\n",
    "    # ============\n",
    "    # Create cluster objects\n",
    "    # ============\n",
    "    ms = cluster.MeanShift(bandwidth=bandwidth, bin_seeding=True)\n",
    "    two_means = cluster.MiniBatchKMeans(n_clusters=params['n_clusters'])\n",
    "    ward = cluster.AgglomerativeClustering(\n",
    "        n_clusters=params['n_clusters'], linkage='ward',\n",
    "        connectivity=connectivity)\n",
    "    spectral = cluster.SpectralClustering(\n",
    "        n_clusters=params['n_clusters'], eigen_solver='arpack',\n",
    "        affinity=\"nearest_neighbors\")\n",
    "    dbscan = cluster.DBSCAN(eps=params['eps'])\n",
    "    affinity_propagation = cluster.AffinityPropagation(\n",
    "        damping=params['damping'], preference=params['preference'])\n",
    "    average_linkage = cluster.AgglomerativeClustering(\n",
    "        linkage=\"average\", affinity=\"cityblock\",\n",
    "        n_clusters=params['n_clusters'], connectivity=connectivity)\n",
    "    birch = cluster.Birch(n_clusters=params['n_clusters'])\n",
    "    gmm = mixture.GaussianMixture(\n",
    "        n_components=params['n_clusters'], covariance_type='full')\n",
    "\n",
    "    clustering_algorithms = (\n",
    "        ('MiniBatchKMeans', two_means),\n",
    "        ('AffinityPropagation', affinity_propagation),\n",
    "        ('MeanShift', ms),\n",
    "        ('SpectralClustering', spectral),\n",
    "        ('Ward', ward),\n",
    "        ('AgglomerativeClustering', average_linkage),\n",
    "        ('DBSCAN', dbscan),\n",
    "        ('Birch', birch),\n",
    "        ('GaussianMixture', gmm)\n",
    "    )\n",
    "\n",
    "    for name, algorithm in clustering_algorithms:\n",
    "        t0 = time.time()\n",
    "\n",
    "        # catch warnings related to kneighbors_graph\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\n",
    "                \"ignore\",\n",
    "                message=\"the number of connected components of the \" +\n",
    "                \"connectivity matrix is [0-9]{1,2}\" +\n",
    "                \" > 1. Completing it to avoid stopping the tree early.\",\n",
    "                category=UserWarning)\n",
    "            warnings.filterwarnings(\n",
    "                \"ignore\",\n",
    "                message=\"Graph is not fully connected, spectral embedding\" +\n",
    "                \" may not work as expected.\",\n",
    "                category=UserWarning)\n",
    "            algorithm.fit(X)\n",
    "\n",
    "        t1 = time.time()\n",
    "        if hasattr(algorithm, 'labels_'):\n",
    "            y_pred = algorithm.labels_.astype(np.int)\n",
    "        else:\n",
    "            y_pred = algorithm.predict(X)\n",
    "\n",
    "        plt.subplot(len(datasets), len(clustering_algorithms), plot_num)\n",
    "        if i_dataset == 0:\n",
    "            plt.title(name, size=18)\n",
    "\n",
    "        colors = np.array(list(islice(cycle(['#377eb8', '#ff7f00', '#4daf4a',\n",
    "                                             '#f781bf', '#a65628', '#984ea3',\n",
    "                                             '#999999', '#e41a1c', '#dede00']),\n",
    "                                      int(max(y_pred) + 1))))\n",
    "        plt.scatter(X[:, 0], X[:, 1], s=10, color=colors[y_pred])\n",
    "\n",
    "        plt.xlim(-2.5, 2.5)\n",
    "        plt.ylim(-2.5, 2.5)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "        plt.text(.99, .01, ('%.2fs' % (t1 - t0)).lstrip('0'),\n",
    "                 transform=plt.gca().transAxes, size=15,\n",
    "                 horizontalalignment='right')\n",
    "        plot_num += 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
